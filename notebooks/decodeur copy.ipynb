{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10414454,"sourceType":"datasetVersion","datasetId":6454527},{"sourceId":10415579,"sourceType":"datasetVersion","datasetId":6455369}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport pickle as pkl\nimport os\nfrom tabulate import tabulate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, LlamaConfig\nimport requests\nfrom huggingface_hub import hf_hub_download\nfrom nltk import download; download('punkt_tab')\n\nimport torch\nimport warnings\n# Disable all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.041433Z","iopub.execute_input":"2025-01-12T18:40:15.041723Z","iopub.status.idle":"2025-01-12T18:40:15.058504Z","shell.execute_reply.started":"2025-01-12T18:40:15.041701Z","shell.execute_reply":"2025-01-12T18:40:15.057779Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#pip install -U bitsandbytes txtai\n# pip install bitsandbytes\n# pip install accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.059386Z","iopub.execute_input":"2025-01-12T18:40:15.059667Z","iopub.status.idle":"2025-01-12T18:40:15.095641Z","shell.execute_reply.started":"2025-01-12T18:40:15.059645Z","shell.execute_reply":"2025-01-12T18:40:15.094934Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.097366Z","iopub.execute_input":"2025-01-12T18:40:15.097634Z","iopub.status.idle":"2025-01-12T18:40:15.110826Z","shell.execute_reply.started":"2025-01-12T18:40:15.097614Z","shell.execute_reply":"2025-01-12T18:40:15.110134Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Set device globally\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Function to move data to device\ndef move_to_device(data, device):\n    if isinstance(data, torch.Tensor):\n        return data.to(device)\n    elif isinstance(data, dict):  # For dicts like inputs\n        return {key: value.to(device) for key, value in data.items()}\n    elif isinstance(data, list):  # For lists of tensors\n        return [move_to_device(item, device) for item in data]\n    else:\n        return data  # If not tensor, return as is\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.112087Z","iopub.execute_input":"2025-01-12T18:40:15.112407Z","iopub.status.idle":"2025-01-12T18:40:15.203574Z","shell.execute_reply.started":"2025-01-12T18:40:15.112384Z","shell.execute_reply":"2025-01-12T18:40:15.202621Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"path_train = \"/kaggle/input/train-data\"\nwith open(f\"{path_train}/train_data.dat\", \"rb\") as f:\n    data_obs_train, data_rct_train = pkl.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.204555Z","iopub.execute_input":"2025-01-12T18:40:15.204878Z","iopub.status.idle":"2025-01-12T18:40:15.515966Z","shell.execute_reply.started":"2025-01-12T18:40:15.204845Z","shell.execute_reply":"2025-01-12T18:40:15.515278Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"path_test = \"/kaggle/input/data-test\"\nwith open(f\"{path_test}/data_test.dat\", \"rb\") as f:\n    data_obs_test, data_rct_test = pkl.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.516704Z","iopub.execute_input":"2025-01-12T18:40:15.516930Z","iopub.status.idle":"2025-01-12T18:40:15.548183Z","shell.execute_reply.started":"2025-01-12T18:40:15.516911Z","shell.execute_reply":"2025-01-12T18:40:15.547610Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Move data to the selected device\ndata_obs_train = move_to_device(data_obs_train, device)\ndata_rct_train = move_to_device(data_rct_train, device)\ndata_obs_test = move_to_device(data_obs_test, device)\ndata_rct_test = move_to_device(data_rct_test, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.548931Z","iopub.execute_input":"2025-01-12T18:40:15.549212Z","iopub.status.idle":"2025-01-12T18:40:15.552891Z","shell.execute_reply.started":"2025-01-12T18:40:15.549184Z","shell.execute_reply":"2025-01-12T18:40:15.552221Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def compute_rouge2(generated, reference):\n    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n    score = scorer.score(reference, generated)\n    return score['rouge2'].fmeasure\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.554831Z","iopub.execute_input":"2025-01-12T18:40:15.555029Z","iopub.status.idle":"2025-01-12T18:40:15.565754Z","shell.execute_reply.started":"2025-01-12T18:40:15.555013Z","shell.execute_reply":"2025-01-12T18:40:15.565074Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Set your Hugging Face API key\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"HF_TOKEN\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.567159Z","iopub.execute_input":"2025-01-12T18:40:15.567411Z","iopub.status.idle":"2025-01-12T18:40:15.793973Z","shell.execute_reply.started":"2025-01-12T18:40:15.567392Z","shell.execute_reply":"2025-01-12T18:40:15.793356Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Define the model name\n\nmodel_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.794626Z","iopub.execute_input":"2025-01-12T18:40:15.794854Z","iopub.status.idle":"2025-01-12T18:40:15.798035Z","shell.execute_reply.started":"2025-01-12T18:40:15.794834Z","shell.execute_reply":"2025-01-12T18:40:15.797173Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=api_key)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:15.798788Z","iopub.execute_input":"2025-01-12T18:40:15.799081Z","iopub.status.idle":"2025-01-12T18:40:17.519737Z","shell.execute_reply.started":"2025-01-12T18:40:15.799052Z","shell.execute_reply":"2025-01-12T18:40:17.519049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9eee92fe4a842018461d542e865bfc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c6ad704cfd4a6087fd8d9cf777c61f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae5155bc1504662a5c0f122365cf83d"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:40:17.520565Z","iopub.execute_input":"2025-01-12T18:40:17.520844Z","iopub.status.idle":"2025-01-12T18:42:58.231669Z","shell.execute_reply.started":"2025-01-12T18:40:17.520813Z","shell.execute_reply":"2025-01-12T18:42:58.230627Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7792f263ddfa4c16b949d2b6084305b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"034d359ab3ac430b8e823f0e436f6f01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e707d524909f4c629540ec4d92b29cc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f637ca1349446fb645f94964e7840b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa6547c270c42ba8c78f37077806930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1fb221de224e4f9a6d1d2df033fa1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f8d7a12fd74ee085b9fb182a9d4c0a"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:42:58.232592Z","iopub.execute_input":"2025-01-12T18:42:58.232976Z","iopub.status.idle":"2025-01-12T18:43:02.342767Z","shell.execute_reply.started":"2025-01-12T18:42:58.232952Z","shell.execute_reply":"2025-01-12T18:43:02.341824Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def split_into_chunks_with_overlap(article, tokenizer, chunk_size=512, overlap_size=100):\n    \"\"\"\n    Splits a long article into chunks with a specified size and overlap.\n\n    Args:\n        article (str): The full article to be summarized.\n        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to count tokens.\n        chunk_size (int): The maximum number of tokens per chunk (default: 512).\n        overlap_size (int): The number of tokens to overlap between chunks (default: 100).\n\n    Returns:\n        list: A list of chunks (strings), each with a token length <= chunk_size, and overlap.\n    \"\"\"\n    # Tokenize the article into token IDs (flat list)\n    tokens = tokenizer(article, truncation=False, padding=False)[\"input_ids\"]\n\n    # Create chunks with overlap\n    chunks = []\n    for i in range(0, len(tokens), chunk_size - overlap_size):\n        chunk = tokens[i:i + chunk_size]\n        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n        chunks.append(chunk_text)\n        \n    return chunks\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:43:10.682218Z","iopub.execute_input":"2025-01-12T18:43:10.682481Z","iopub.status.idle":"2025-01-12T18:43:10.695492Z","shell.execute_reply.started":"2025-01-12T18:43:10.682461Z","shell.execute_reply":"2025-01-12T18:43:10.694818Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def generate_abstract_from_prompt(article, tokenizer, model, device, prompt, chunk_size=2048, overlap_size=100, max_new_tokens=300):\n    \"\"\"\n    Generates an abstract for an article by processing it in chunks and summarizing each chunk.\n\n    Args:\n        article (str): The full article to be summarized.\n        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to count tokens.\n        model (transformers.PreTrainedModel): The pre-trained model used for generating the summary.\n        device (torch.device): The device on which to perform the computation ('cpu' or 'cuda').\n        prompt (str): The prompt to guide the summarization process.\n        chunk_size (int): The maximum number of tokens per chunk (default: 2048).\n        overlap_size (int): The number of tokens to overlap between chunks (default: 100).\n\n    Returns:\n        str: The generated abstract for the full article.\n    \"\"\"\n    # Check and set the pad_token if not already defined\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n\n    # Split the article into chunks with overlap\n    chunks = split_into_chunks_with_overlap(article, tokenizer, chunk_size, overlap_size)\n\n    # Generate abstract for each chunk\n    abstracts = []\n    for i, chunk in enumerate(chunks):\n        # Combine the provided prompt with the chunk\n        full_prompt = f\"{prompt} {chunk}\"\n        \n        # Tokenize the prompt and generate the summary\n        inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=chunk_size, return_attention_mask=True)\n\n        # Move input tensors to the same device as the model\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        \n        # Fix the pad_token_id issue and generate the output\n        outputs = model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_new_tokens=max_new_tokens,  # Make sure this is an integer\n            num_beams=1,  # Integer, not a list\n            early_stopping=True,\n            pad_token_id=model.config.eos_token_id[0]  # Explicitly set pad_token_id to eos_token_id\n        )\n        \n        # Decode the generated summary and remove the prompt from the generated text\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Remove the prompt part from the generated abstract\n        abstract = generated_text[len(full_prompt):].strip()  # Strip the prompt text\n        \n        abstracts.append(abstract)\n        print(f\"Chunk number {i} has been processed\")\n\n    # Join all the small abstracts into one big abstract\n    full_abstract = \" \".join(abstracts)\n    \n    return full_abstract\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:50:42.556644Z","iopub.execute_input":"2025-01-12T18:50:42.556966Z","iopub.status.idle":"2025-01-12T18:50:42.564422Z","shell.execute_reply.started":"2025-01-12T18:50:42.556944Z","shell.execute_reply":"2025-01-12T18:50:42.563495Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def calculate_rouge_scores(generated_summary, true_abstract):\n    \"\"\"\n    Calculates and prints ROUGE scores for the generated summary compared to the true abstract.\n\n    Args:\n        generated_summary (str): The generated summary text.\n        true_abstract (str): The true abstract text.\n    \"\"\"\n    # Initialize ROUGE scorer\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n    # Compare the generated summary with the true abstract\n    scores = scorer.score(true_abstract, generated_summary)\n\n    return scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:43:10.714098Z","iopub.execute_input":"2025-01-12T18:43:10.714400Z","iopub.status.idle":"2025-01-12T18:43:10.729392Z","shell.execute_reply.started":"2025-01-12T18:43:10.714369Z","shell.execute_reply":"2025-01-12T18:43:10.728586Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"prompt = \"\"\"\nYou are a summarizer encoder-decoder model. The following is a chunk of a scientific article in the field of medicine. Give an extractive abstract of this chunk.\n\"\"\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:48:00.126932Z","iopub.execute_input":"2025-01-12T18:48:00.127315Z","iopub.status.idle":"2025-01-12T18:48:00.131205Z","shell.execute_reply.started":"2025-01-12T18:48:00.127287Z","shell.execute_reply":"2025-01-12T18:48:00.130347Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Load the input article and true abstract\narticle_1 = data_obs_train.iloc[0, 1] # Input article\ntrue_abstract_1 = data_obs_train.iloc[0, 2]  # True abstract\narticle_2 = data_obs_train.iloc[1, 1] # Input article\ntrue_abstract_2 = data_obs_train.iloc[1, 2]  # True abstract\narticle_3 = data_obs_train.iloc[2, 1] # Input article\ntrue_abstract_3 = data_obs_train.iloc[2, 2]  # True abstract\narticle_4 = data_obs_train.iloc[3, 1] # Input article\ntrue_abstract_4 = data_obs_train.iloc[3, 2]  # True abstract\narticle_5 = data_obs_train.iloc[4, 1] # Input article\ntrue_abstract_5 = data_obs_train.iloc[4, 2]  # True abstract\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T19:37:59.972020Z","iopub.execute_input":"2025-01-12T19:37:59.972391Z","iopub.status.idle":"2025-01-12T19:37:59.977791Z","shell.execute_reply.started":"2025-01-12T19:37:59.972360Z","shell.execute_reply":"2025-01-12T19:37:59.976860Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()  # Free memory on GPU (if applicable)\nimport gc\ngc.collect()  # Force garbage collection to free unused memory\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T18:43:10.760751Z","iopub.execute_input":"2025-01-12T18:43:10.760963Z","iopub.status.idle":"2025-01-12T18:43:10.926374Z","shell.execute_reply.started":"2025-01-12T18:43:10.760944Z","shell.execute_reply":"2025-01-12T18:43:10.925687Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"38"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"device = torch.device('cuda')\ngenerated_abstract_1 = generate_abstract_from_prompt(article_1, tokenizer, model, device, prompt=prompt, chunk_size=2048, max_new_tokens=350)\ngenerated_abstract_2 = generate_abstract_from_prompt(article_2, tokenizer, model, device, prompt=prompt, chunk_size=2048, max_new_tokens=350)\ngenerated_abstract_3 = generate_abstract_from_prompt(article_3, tokenizer, model, device, prompt=prompt, chunk_size=2048, max_new_tokens=350)\ngenerated_abstract_4 = generate_abstract_from_prompt(article_4, tokenizer, model, device, prompt=prompt, chunk_size=2048, max_new_tokens=350)\ngenerated_abstract_5 = generate_abstract_from_prompt(article_5, tokenizer, model, device, prompt=prompt, chunk_size=2048, max_new_tokens=350)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T19:39:10.544453Z","iopub.execute_input":"2025-01-12T19:39:10.544752Z","iopub.status.idle":"2025-01-12T19:49:33.998623Z","shell.execute_reply.started":"2025-01-12T19:39:10.544731Z","shell.execute_reply":"2025-01-12T19:49:33.997646Z"}},"outputs":[{"name":"stdout","text":"Chunk number 0 has been processed\nChunk number 1 has been processed\nChunk number 2 has been processed\nChunk number 0 has been processed\nChunk number 1 has been processed\nChunk number 2 has been processed\nChunk number 3 has been processed\nChunk number 0 has been processed\nChunk number 1 has been processed\nChunk number 2 has been processed\nChunk number 3 has been processed\nChunk number 0 has been processed\nChunk number 1 has been processed\nChunk number 2 has been processed\nChunk number 3 has been processed\nChunk number 4 has been processed\nChunk number 0 has been processed\nChunk number 1 has been processed\nChunk number 2 has been processed\nChunk number 3 has been processed\nChunk number 4 has been processed\nChunk number 5 has been processed\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"score_1 = calculate_rouge_scores(generated_abstract, true_abstract)\nscore_2 = calculate_rouge_scores(generated_abstract, true_abstract)\nscore_3 = calculate_rouge_scores(generated_abstract, true_abstract)\nscore_4 = calculate_rouge_scores(generated_abstract, true_abstract)\nscore_5 = calculate_rouge_scores(generated_abstract, true_abstract)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T19:50:52.546223Z","iopub.execute_input":"2025-01-12T19:50:52.546620Z","iopub.status.idle":"2025-01-12T19:50:52.887763Z","shell.execute_reply.started":"2025-01-12T19:50:52.546591Z","shell.execute_reply":"2025-01-12T19:50:52.886843Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"# Liste de scores obtenus à partir de la fonction calculate_rouge_scores\nall_scores = [score_1, score_2, score_3, score_4, score_5]\n\n# Initialiser les totaux pour chaque métrique\nrouge1_total = 0\nrouge2_total = 0\nrougeL_total = 0\n\n# Ajouter les scores pour chaque appel\nfor score in all_scores:\n    rouge1_total += score['rouge1'].fmeasure\n    rouge2_total += score['rouge2'].fmeasure\n    rougeL_total += score['rougeL'].fmeasure\n\n# Calculer la moyenne pour chaque score\nrouge1_avg = rouge1_total / len(all_scores)\nrouge2_avg = rouge2_total / len(all_scores)\nrougeL_avg = rougeL_total / len(all_scores)\nC:\\Users\\medal\\Downloads\\nlp-mohammedalieladlouni-decodeur (2).ipynb\n# Afficher les moyennes\nprint(f\"ROUGE-1 Average: {rouge1_avg}\")\nprint(f\"ROUGE-2 Average: {rouge2_avg}\")\nprint(f\"ROUGE-L Average: {rougeL_avg}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T19:50:56.085761Z","iopub.execute_input":"2025-01-12T19:50:56.086071Z","iopub.status.idle":"2025-01-12T19:50:56.092450Z","shell.execute_reply.started":"2025-01-12T19:50:56.086045Z","shell.execute_reply":"2025-01-12T19:50:56.091557Z"}},"outputs":[{"name":"stdout","text":"ROUGE-1 Average: 0.3466666666666667\nROUGE-2 Average: 0.13140311804008908\nROUGE-L Average: 0.1577777777777778\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}